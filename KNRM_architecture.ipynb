{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9b8f9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T08:43:07.009825Z",
     "start_time": "2022-05-03T08:43:06.876508Z"
    }
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# from tqdm.notebook import tqdm\n",
    "# tqdm.pandas()\n",
    "# import os\n",
    "# import timeit\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "glue_qqp_dir = 'data/QQP/'\n",
    "glove_path = 'data/glove.6B.50d.txt'\n",
    "\n",
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1., sigma: float = 1.):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.exp( (-(x-self.mu)**2)/(2*self.sigma**2) ) \n",
    "\n",
    "class KNRM(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix: np.ndarray, freeze_embeddings: bool, kernel_num: int = 21,\n",
    "                 sigma: float = 0.1, exact_sigma: float = 0.001,\n",
    "                 out_layers: List[int] = [10, 5]):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=freeze_embeddings,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.kernel_num = kernel_num\n",
    "        self.sigma = sigma\n",
    "        self.exact_sigma = exact_sigma\n",
    "        self.out_layers = out_layers\n",
    "\n",
    "        self.kernels = self._get_kernels_layers()\n",
    "\n",
    "        self.mlp = self._get_mlp()\n",
    "\n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
    "        kernels = torch.nn.ModuleList()\n",
    "        K = self.kernel_num\n",
    "        step = 1 / (K - 1)\n",
    "        ar_1 = np.linspace(step, 1-step, (K-1)//2, endpoint=True)\n",
    "        ar_2 = -ar_1[::-1]\n",
    "        ar = np.hstack((ar_2, ar_1, np.array([1])))\n",
    "        mu_list = torch.from_numpy(ar)\n",
    "        sigma_list = [self.sigma] * (K - 1) + [self.exact_sigma]\n",
    "        kernels = torch.nn.ModuleList([GaussianKernel(mu, sigma) for mu, sigma in zip(mu_list, sigma_list)])\n",
    "        return kernels\n",
    "\n",
    "    def _get_mlp(self) -> torch.nn.Sequential:\n",
    "        K = self.kernel_num\n",
    "        model = torch.nn.Sequential()\n",
    "        if len(self.out_layers) > 0:\n",
    "            for i, count_layers in enumerate(self.out_layers):\n",
    "                model.add_module(str(i), torch.nn.Linear(K, count_layers))\n",
    "                model.add_module(torch.nn.ReLU())\n",
    "            model.add_module(str(i+1), torch.nn.Linear(count_layers[-1], 1))\n",
    "        else:\n",
    "            model.add_module('0', torch.nn.Linear(K, 1))\n",
    "        return model\n",
    "\n",
    "    def forward(self, input_1: Dict[str, torch.Tensor], input_2: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        logits_1 = self.predict(input_1)\n",
    "        logits_2 = self.predict(input_2)\n",
    "\n",
    "        logits_diff = logits_1 - logits_2\n",
    "\n",
    "        out = self.out_activation(logits_diff)\n",
    "        return out\n",
    "\n",
    "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
    "        embedding_query = self.embeddings(query.long())   \n",
    "        embedding_doc = self.embeddings(doc.long())\n",
    "        matching_matrix = torch.einsum('bld,brd->blr',\n",
    "                                        F.normalize(embedding_query, p=2, dim=-1),\n",
    "                                        F.normalize(embedding_doc, p=2, dim=-1))\n",
    "        return matching_matrix\n",
    "\n",
    "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        KM = []\n",
    "        for kernel in self.kernels:\n",
    "            # shape = [B]\n",
    "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "            KM.append(K)\n",
    "\n",
    "        # shape = [B, K]\n",
    "        kernels_out = torch.stack(KM, dim=1)\n",
    "        return kernels_out\n",
    "\n",
    "    def predict(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        # shape = [Batch, Left, Embedding], [Batch, Right, Embedding]\n",
    "        query, doc = inputs['query'], inputs['document']\n",
    "\n",
    "        # shape = [Batch, Left, Right]\n",
    "        matching_matrix = self._get_matching_matrix(query, doc)\n",
    "        # shape = [Batch, Kernels]\n",
    "        kernels_out = self._apply_kernels(matching_matrix)\n",
    "        # shape = [Batch]\n",
    "        out = self.mlp(kernels_out)\n",
    "        return out\n",
    "\n",
    "class RankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, index_pairs_or_triplets: List[List[Union[str, float]]],\n",
    "                 idx_to_text_mapping: Dict[str, str], vocab: Dict[str, int], oov_val: int,\n",
    "                 preproc_func: Callable, max_len: int = 30):\n",
    "        self.index_pairs_or_triplets = index_pairs_or_triplets\n",
    "        self.idx_to_text_mapping = idx_to_text_mapping\n",
    "        self.vocab = vocab\n",
    "        self.oov_val = oov_val\n",
    "        self.preproc_func = preproc_func\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_pairs_or_triplets)\n",
    "\n",
    "    def _tokenized_text_to_index(self, tokenized_text: List[str]) -> List[int]:\n",
    "        tokenized_list = []\n",
    "        for el in tokenized_text:\n",
    "            word_in_dict = self.vocab.get(el)\n",
    "            if word_in_dict:\n",
    "                tokenized_list.append(word_in_dict)\n",
    "            else:\n",
    "                tokenized_list.append(self.oov_val)\n",
    "            \n",
    "        return tokenized_list\n",
    "\n",
    "    def _convert_text_idx_to_token_idxs(self, idx: str) -> List[int]:\n",
    "        list_with_cleaned_text = self.preproc_func(self.idx_to_text_mapping[idx])\n",
    "        tokenized_text_to_index = self._tokenized_text_to_index(list_with_cleaned_text)\n",
    "        return tokenized_text_to_index\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        pass\n",
    "\n",
    "class TrainTripletsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx): \n",
    "        query_id, document_id_1, document_id_2, answer = self.index_pairs_or_triplets[idx]\n",
    "        query = self._convert_text_idx_to_token_idxs(query_id)[:self.max_len]\n",
    "        doc_1 = self._convert_text_idx_to_token_idxs(document_id_1)[:self.max_len]\n",
    "        doc_2 = self._convert_text_idx_to_token_idxs(document_id_2)[:self.max_len]\n",
    "        return ({\"query\": query,\n",
    "                 \"document\": doc_1},\n",
    "                {\"query\": query,\n",
    "                 \"document\": doc_2},               \n",
    "                 answer)   \n",
    "\n",
    "class ValPairsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        query_id, document_id, answer = self.index_pairs_or_triplets[idx]\n",
    "        query = self._convert_text_idx_to_token_idxs(query_id)[:self.max_len]\n",
    "        doc = self._convert_text_idx_to_token_idxs(document_id)[:self.max_len]\n",
    "        return ({\"query\": query, \"document\": doc}, answer)\n",
    "\n",
    "def collate_fn(batch_objs: List[Union[Dict[str, torch.Tensor], torch.FloatTensor]]):\n",
    "    max_len_q1 = -1\n",
    "    max_len_d1 = -1\n",
    "    max_len_q2 = -1\n",
    "    max_len_d2 = -1\n",
    "\n",
    "    is_triplets = False\n",
    "    for elem in batch_objs:\n",
    "        if len(elem) == 3:\n",
    "            left_elem, right_elem, label = elem\n",
    "            is_triplets = True\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        max_len_q1 = max(len(left_elem['query']), max_len_q1)\n",
    "        max_len_d1 = max(len(left_elem['document']), max_len_d1)\n",
    "        if len(elem) == 3:\n",
    "            max_len_q2 = max(len(right_elem['query']), max_len_q2)\n",
    "            max_len_d2 = max(len(right_elem['document']), max_len_d2)\n",
    "\n",
    "    q1s = []\n",
    "    d1s = []\n",
    "    q2s = []\n",
    "    d2s = []\n",
    "    labels = []\n",
    "    \n",
    "    for elem in batch_objs:\n",
    "        if is_triplets:\n",
    "            left_elem, right_elem, label = elem\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        pad_len1 = max_len_q1 - len(left_elem['query'])\n",
    "        pad_len2 = max_len_d1 - len(left_elem['document'])\n",
    "        if is_triplets:\n",
    "            pad_len3 = max_len_q2 - len(right_elem['query'])\n",
    "            pad_len4 = max_len_d2 - len(right_elem['document'])\n",
    "\n",
    "        q1s.append(left_elem['query'] + [0] * pad_len1)\n",
    "        d1s.append(left_elem['document'] + [0] * pad_len2)\n",
    "        if is_triplets:\n",
    "            q2s.append(right_elem['query'] + [0] * pad_len3)\n",
    "            d2s.append(right_elem['document'] + [0] * pad_len4)\n",
    "        labels.append([label])\n",
    "    q1s = torch.LongTensor(q1s)\n",
    "    d1s = torch.LongTensor(d1s)\n",
    "    if is_triplets:\n",
    "        q2s = torch.LongTensor(q2s)\n",
    "        d2s = torch.LongTensor(d2s)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    ret_left = {'query': q1s, 'document': d1s}\n",
    "    if is_triplets:\n",
    "        ret_right = {'query': q2s, 'document': d2s}\n",
    "        return ret_left, ret_right, labels\n",
    "    else:\n",
    "        return ret_left, labels\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, glue_qqp_dir: str, glove_vectors_path: str,\n",
    "                 min_token_occurancies: int = 1,\n",
    "                 random_seed: int = 0,\n",
    "                 emb_rand_uni_bound: float = 0.2,\n",
    "                 freeze_knrm_embeddings: bool = True,\n",
    "                 knrm_kernel_num: int = 21,\n",
    "                 knrm_out_mlp: List[int] = [],\n",
    "                 dataloader_bs: int = 1024,\n",
    "                 train_lr: float = 0.001,\n",
    "                 change_train_loader_ep: int = 10\n",
    "                 ):\n",
    "        self.trans = str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "        self.glue_qqp_dir = glue_qqp_dir\n",
    "        self.glove_vectors_path = glove_vectors_path\n",
    "        self.glue_train_df = self.get_glue_df('train')\n",
    "        self.glue_dev_df = self.get_glue_df('dev')\n",
    "        self.dev_pairs_for_ndcg = self.create_val_pairs(self.glue_dev_df)\n",
    "        self.min_token_occurancies = min_token_occurancies\n",
    "        self.all_tokens = self.get_all_tokens(\n",
    "            [self.glue_train_df, self.glue_dev_df], self.min_token_occurancies)\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        self.emb_rand_uni_bound = emb_rand_uni_bound\n",
    "        self.freeze_knrm_embeddings = freeze_knrm_embeddings\n",
    "        self.knrm_kernel_num = knrm_kernel_num\n",
    "        self.knrm_out_mlp = knrm_out_mlp\n",
    "        self.dataloader_bs = dataloader_bs\n",
    "        self.train_lr = train_lr\n",
    "        self.change_train_loader_ep = change_train_loader_ep\n",
    "\n",
    "        self.model, self.vocab, self.unk_words = self.build_knrm_model()\n",
    "        self.idx_to_text_mapping_train = self.get_idx_to_text_mapping(\n",
    "            self.glue_train_df)\n",
    "        self.idx_to_text_mapping_dev = self.get_idx_to_text_mapping(\n",
    "            self.glue_dev_df)\n",
    "        self.val_dataset = ValPairsDataset(self.dev_pairs_for_ndcg, \n",
    "              self.idx_to_text_mapping_dev, \n",
    "              vocab=self.vocab, oov_val=self.vocab['OOV'], \n",
    "              preproc_func=self.simple_preproc)\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(\n",
    "            self.val_dataset, batch_size=self.dataloader_bs, num_workers=0, \n",
    "            collate_fn=collate_fn, shuffle=False)\n",
    "        \n",
    "    def get_glue_df(self, partition_type: str) -> pd.DataFrame:\n",
    "        assert partition_type in ['dev', 'train']\n",
    "        glue_df = pd.read_csv(\n",
    "            self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', error_bad_lines=False, dtype=object)\n",
    "        glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "        \n",
    "        glue_df_fin = pd.DataFrame({\n",
    "            'id_left': glue_df['qid1'],\n",
    "            'id_right': glue_df['qid2'],\n",
    "            'text_left': glue_df['question1'],\n",
    "            'text_right': glue_df['question2'],\n",
    "            'label': glue_df['is_duplicate'].astype(int)\n",
    "        })\n",
    "        return glue_df_fin\n",
    "\n",
    "    def hadle_punctuation(self, inp_str: str) -> str:\n",
    "#         import re\n",
    "#         return re.sub(f\"[{string.punctuation}]\", \" \", inp_str)\n",
    "        return inp_str.translate(self.trans)\n",
    "\n",
    "    def simple_preproc(self, inp_str: str) -> List[str]:\n",
    "#         nltk.download(\"stopwords\")\n",
    "#         nltk.download(\"wordnet\")\n",
    "#         stopWords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "#         tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "#         wnl = nltk.WordNetLemmatizer()\n",
    "#         inp_str = self.hadle_punctuation(inp_str)\n",
    "#         return [wnl.lemmatize(word) for word in tokenizer.tokenize(inp_str.lower()) if word not in stopWords]\n",
    "#         OR\n",
    "        return nltk.word_tokenize(self.hadle_punctuation(inp_str.lower().strip()))\n",
    "    \n",
    "    def _filter_rare_words(self, vocab: Dict[str, int], min_occurancies: int) -> Dict[str, int]:\n",
    "        vocab = np.array(list(vocab.items()))\n",
    "        vocab = vocab[vocab[:, 1].astype(int) >= min_occurancies]\n",
    "        return dict(vocab)\n",
    "    \n",
    "    def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "        questions = pd.concat([\n",
    "                                list_of_df[0]['text_left'],\n",
    "                                list_of_df[0]['text_right'],\n",
    "                                list_of_df[1]['text_left'],\n",
    "                                list_of_df[1]['text_right'],\n",
    "                              ]).unique()\n",
    "        tokens = self.simple_preproc(' '.join(questions))\n",
    "        unique, counts = np.unique(tokens, return_counts=True)\n",
    "        return list(self._filter_rare_words(dict(zip(unique, counts)), min_occurancies).keys())\n",
    "    \n",
    "#         OR THIS\n",
    "#     def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "#         def flatten(t): return [item for sublist in t for item in sublist]\n",
    "        \n",
    "#         tokens = []\n",
    "#         for df in list_of_df:\n",
    "#             unique_texts = set(\n",
    "#                 df[['text_left', 'text_right']].values.reshape(-1))\n",
    "#             df_tokens = flatten(map(self.simple_preproc, unique_texts))\n",
    "#             tokens.extend(list(df_tokens))\n",
    "            \n",
    "#         count_filtered = self._filter_rare_words(Counter(tokens), min_occurancies)\n",
    "#         return list(count_filtered.keys())\n",
    "\n",
    "    def _read_glove_embeddings(self, file_path: str) -> Dict[str, List[str]]:\n",
    "        corpus = {}\n",
    "        with open(file_path, \"rb\") as glove_embdgs:\n",
    "            for line in glove_embdgs:\n",
    "                word, *vector = line.split()\n",
    "                corpus[word.decode('utf-8')] = vector\n",
    "        return corpus\n",
    "\n",
    "    def create_glove_emb_from_file(self, file_path: str, inner_keys: List[str],\n",
    "                                   random_seed: int, rand_uni_bound: float\n",
    "                                   ) -> Tuple[np.ndarray, Dict[str, int], List[str]]: \n",
    "        np.random.seed(random_seed)\n",
    "        corpus = self._read_glove_embeddings(file_path)\n",
    "        size = 50\n",
    "        inner_keys = ['PAD', 'OOV'] + inner_keys\n",
    "        emb_matrix = []\n",
    "        unk_words = []\n",
    "        vocab = dict()\n",
    "        for i, token in enumerate(inner_keys, start=0):\n",
    "            if token not in corpus.keys():\n",
    "                unk_words.append(token)\n",
    "                embedding = np.random.uniform(-rand_uni_bound, rand_uni_bound, size=size)\n",
    "            else:\n",
    "                embedding = np.array(list(map(float, corpus[token])), dtype=np.float32)\n",
    "            emb_matrix.append(embedding)\n",
    "            vocab[token] = i\n",
    "        emb_matrix[0] = np.zeros(size, dtype=np.float32)\n",
    "        emb_matrix = np.asarray(emb_matrix, dtype=np.float32)\n",
    "        \n",
    "        return emb_matrix, vocab, unk_words\n",
    "\n",
    "    def build_knrm_model(self) -> Tuple[torch.nn.Module, Dict[str, int], List[str]]:\n",
    "        emb_matrix, vocab, unk_words = self.create_glove_emb_from_file(\n",
    "            self.glove_vectors_path, self.all_tokens, self.random_seed, self.emb_rand_uni_bound)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        knrm = KNRM(emb_matrix, freeze_embeddings=self.freeze_knrm_embeddings,\n",
    "                    out_layers=self.knrm_out_mlp, kernel_num=self.knrm_kernel_num)\n",
    "        return knrm, vocab, unk_words\n",
    "\n",
    "    def create_val_pairs(self, inp_df: pd.DataFrame, fill_top_to: int = 15,\n",
    "                         min_group_size: int = 2, seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "        glue_dev_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_dev_leftids_to_use)].groupby('id_left')\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "\n",
    "        out_pairs = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        for id_left, group in groups:\n",
    "            ones_ids = group[group.label > 0].id_right.values\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                cur_chosen = set(ones_ids).union(\n",
    "                    set(zeroes_ids)).union({id_left})\n",
    "                pad_sample = np.random.choice(\n",
    "                    list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            for i in ones_ids:\n",
    "                out_pairs.append([id_left, i, 2])\n",
    "            for i in zeroes_ids:\n",
    "                out_pairs.append([id_left, i, 1])\n",
    "            for i in pad_sample:\n",
    "                out_pairs.append([id_left, i, 0])\n",
    "        return out_pairs\n",
    "\n",
    "    def get_idx_to_text_mapping(self, inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "        left_dict = (\n",
    "            inp_df\n",
    "            [['id_left', 'text_left']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_left')\n",
    "            ['text_left']\n",
    "            .to_dict()\n",
    "        )\n",
    "        right_dict = (\n",
    "            inp_df\n",
    "            [['id_right', 'text_right']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_right')\n",
    "            ['text_right']\n",
    "            .to_dict()\n",
    "        )\n",
    "        left_dict.update(right_dict)\n",
    "        return left_dict\n",
    "\n",
    "    def _dcg(self, ys_true: np.array, ys_pred: np.array, top_k: int) -> float:\n",
    "        idxs = np.argsort(ys_pred)[::-1]\n",
    "        ys_true_sorted = ys_true[idxs]\n",
    "        ys_true_sorted = ys_true_sorted[:top_k]\n",
    "\n",
    "        result: float = 0\n",
    "        for k, el in enumerate(ys_true_sorted): \n",
    "            result += (2**el - 1)/math.log2(k+2)\n",
    "        return result\n",
    "    \n",
    "    def ndcg_k(self, ys_true: np.array, ys_pred: np.array, ndcg_top_k: int = 10) -> float:\n",
    "        nom = self._dcg(ys_true.flatten(), ys_pred.flatten(), ndcg_top_k)\n",
    "        denom = self._dcg(ys_true.flatten(), ys_true.flatten(), ndcg_top_k)\n",
    "        result: float = nom/denom\n",
    "        if float(denom.item()) == 0.0:\n",
    "            return float(0.0)\n",
    "        return float(result.item())\n",
    "        \n",
    "    def valid(self, model: torch.nn.Module, val_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        labels_and_groups = val_dataloader.dataset.index_pairs_or_triplets\n",
    "        labels_and_groups = pd.DataFrame(labels_and_groups, columns=['left_id', 'right_id', 'rel'])\n",
    "        \n",
    "        all_preds = []\n",
    "        for batch in (val_dataloader):\n",
    "            inp_1, y = batch\n",
    "            preds = model.predict(inp_1)\n",
    "            preds_np = preds.detach().numpy()\n",
    "            all_preds.append(preds_np)\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        labels_and_groups['preds'] = all_preds\n",
    "        \n",
    "        ndcgs = []\n",
    "        for cur_id in labels_and_groups.left_id.unique():\n",
    "            cur_df = labels_and_groups[labels_and_groups.left_id == cur_id]\n",
    "            ndcg = self.ndcg_k(cur_df.rel.values.reshape(-1), cur_df.preds.values.reshape(-1))\n",
    "            if np.isnan(ndcg):\n",
    "                ndcgs.append(0)\n",
    "            else:\n",
    "                ndcgs.append(ndcg)\n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "    def sample_data_for_train_iter(self, inp_df: pd.DataFrame, seed: int\n",
    "                                   ) -> List[List[Union[str, float]]]:\n",
    "        train_pairs = self.create_val_pairs(inp_df, seed=seed)\n",
    "        df = pd.DataFrame(train_pairs, columns=[\"query\", \"question\", \"ans\"])\n",
    "        positive_triplets = list()\n",
    "        negative_triplets = list()\n",
    "        for query in df[\"query\"].unique():\n",
    "            positive = df.loc[((df[\"query\"] == query) & (df[\"ans\"] > 0)), \"question\"].unique()\n",
    "            negative = df.loc[((df[\"query\"] == query) & (df[\"ans\"] == 0)), \"question\"].unique()\n",
    "            for pos_quest in positive:\n",
    "                for neg_quest in negative:\n",
    "                    positive_triplets.append([query, pos_quest, neg_quest, 1])\n",
    "                    negative_triplets.append([query, neg_quest, pos_quest, 0])\n",
    "                \n",
    "            positive_with_1 = df.loc[((df[\"query\"] == query) & (df[\"ans\"] == 1)), \"question\"].unique()\n",
    "            positive_with_2 = df.loc[((df[\"query\"] == query) & (df[\"ans\"] == 2)), \"question\"].unique()\n",
    "            for pos_quest in positive_with_2:\n",
    "                for neg_quest in positive_with_1:\n",
    "                    positive_triplets.append([query, pos_quest, neg_quest, 1])\n",
    "                    negative_triplets.append([query, neg_quest, pos_quest, 0])\n",
    "\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "        sample_count = self.change_train_loader_ep * self.dataloader_bs // 2\n",
    "        triples = random.sample(positive_triplets, sample_count) + random.sample(negative_triplets, sample_count)\n",
    "        random.shuffle(triples)\n",
    "        return triples\n",
    "\n",
    "    def train(self, n_epochs: int):\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=self.train_lr)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        ndcgs = []\n",
    "        for epoch in range(n_epochs):\n",
    "#             start_time = timeit.default_timer()\n",
    "            self.model.train()\n",
    "            if epoch % self.change_train_loader_ep == 0:\n",
    "                self.sample_size = 20000\n",
    "                start_pos = (epoch // self.change_train_loader_ep) % (self.glue_train_df.shape[0] // self.sample_size - 1)\n",
    "                triplets = self.sample_data_for_train_iter(\\\n",
    "                    self.glue_train_df.iloc[start_pos*self.sample_size:(start_pos+1)*self.sample_size], seed=epoch)\n",
    "                train_dataset = TrainTripletsDataset(triplets, \n",
    "                                                     self.idx_to_text_mapping_train, \n",
    "                                                     vocab=self.vocab, oov_val=self.vocab['OOV'], \n",
    "                                                     preproc_func=self.simple_preproc)      \n",
    "                train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                               batch_size=self.dataloader_bs, \n",
    "                                                               num_workers=0, \n",
    "                                                               collate_fn=collate_fn, shuffle=True)\n",
    "                self.w = train_dataloader\n",
    "            for query, question, ans in train_dataloader:\n",
    "                loss = criterion(self.model(query, question), ans)\n",
    "                loss.backward()\n",
    "                opt.step()     \n",
    "            with torch.no_grad():\n",
    "                self.model.eval()\n",
    "                valuation = self.valid(self.model, self.val_dataloader)\n",
    "                ndcgs.append(valuation)\n",
    "                print(f'Epoch: {epoch+1} Valuation: {valuation}')\n",
    "    #                 print(\"epoch timing\", timeit.default_timer() - start_time)\n",
    "                if valuation >= 0.925:\n",
    "                    break\n",
    "        return ndcgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "808fca8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T08:44:26.336056Z",
     "start_time": "2022-05-03T08:43:07.359134Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = Solution(glue_qqp_dir, glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ce82918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-03T08:45:56.801334Z",
     "start_time": "2022-05-03T08:44:26.343780Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Valuation: 0.5602665132253382\n",
      "Epoch: 2 Valuation: 0.65231762985748\n",
      "Epoch: 3 Valuation: 0.9290850132255103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5602665132253382, 0.65231762985748, 0.9290850132255103]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.train(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edec065a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T08:32:36.949469Z",
     "start_time": "2022-05-01T08:32:36.795465Z"
    }
   },
   "outputs": [],
   "source": [
    "state_mlp = m.model.mlp.state_dict()\n",
    "torch.save(state_mlp, open('outputs3/knrm_mlp.bin', 'wb'))\n",
    "\n",
    "state_emb = m.model.embeddings.state_dict()\n",
    "torch.save(state_emb, open('outputs3/knrm_emb.bin', 'wb'))\n",
    "\n",
    "import json\n",
    "state_vocab = m.vocab\n",
    "json.dump(state_vocab, open('outputs3/vocab.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
